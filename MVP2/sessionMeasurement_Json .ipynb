{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00W7lv-MvmZP"
      },
      "source": [
        "## Lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwdWhBYqvn2r",
        "outputId": "ea352d44-39cd-4309-8f5b-9156b05b9659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==1.2.0\n",
            "  Downloading openai-1.2.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.9/219.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.2.0) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.2.0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (2.7.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.2.0) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.2.0) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.2.0) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.2.0) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.2.0) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.2.0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.2.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.2.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.2.0) (2.18.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==1.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA4W76MFvqGJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from collections import Counter\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvpJrolZvrZr"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "  api_key=\"Your OpenAI API here\"\n",
        ")\n",
        "def get_completion(prompt, model = \"gpt-3.5-turbo\", temperature =0):\n",
        "  messages = [{\"role\":\"user\", \"content\": prompt}]\n",
        "  response = client.chat.completions.create(\n",
        "      model = model,\n",
        "      messages = messages,\n",
        "      temperature =temperature # degree of expiration (randomness) (0 same result - 1 creative)\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQCn8L0bvspO",
        "outputId": "3d0b6550-c28b-4ff9-b8fc-67d67b4dca84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGDBxu_nwHOo"
      },
      "source": [
        "## Technical Issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuiYjS9dOFCb"
      },
      "outputs": [],
      "source": [
        "def tech(data):\n",
        "  duration = data['Utterance end time (milliseconds)'].iloc[-1]\n",
        "  start = 0\n",
        "  end = 1\n",
        "  l = []\n",
        "  idx = []\n",
        "  num = data['Utterance end time (milliseconds)'].iloc[-1]/60000/20\n",
        "  for i in range(len(data)):\n",
        "    if data['Utterance end time (milliseconds)'].iloc[i] > (end*60000*num):\n",
        "        d = data[['Speaker', 'Utterance']].iloc[start:i+1].to_json()\n",
        "        prompt = f\"\"\"I provided you a chunk of transcript between a student and a tutor in a online one-on-one tutoring session{d}.\n",
        "        You task is to identidy that whether the tutor is experiencing a technnological issue with the plateform.\n",
        "        Here are some examples of technique issue: donot know how to use a tool, cannot hear each other, cannot see each other, cannot upload a file, cannot share the screen, or cannot see the screen.\n",
        "        Output only 1 for yes or 0 for no.\"\"\"\n",
        "        r = get_completion(prompt)\n",
        "        l.append(r[0])\n",
        "        idx.append([start,i+1])\n",
        "        start = i+1\n",
        "        end = end+1\n",
        "  d = data[['Speaker', 'Utterance']].iloc[start:len(data)-1].to_json()\n",
        "  prompt = f\"\"\"I provided you a chunk of transcript between a student and a tutor in a online one-on-one tutoring session{d}.\n",
        "        You task is to identidy that whether the tutor is experiencing a technical issure.\n",
        "        Here are some examples of technique issue: donot know how to use a tool, cannot hear each other, cannot see each other, cannot upload a file, cannot share the screen, or cannot see the screen.\n",
        "        Output only 1 for yes or 0 for no.\"\"\"\n",
        "  r = get_completion(prompt)\n",
        "  l.append(r[0])\n",
        "  idx.append([start,i+1])\n",
        "\n",
        "  total_tech_score = sum(list(map(int, l)))/len(l)\n",
        "  return(l, idx, total_tech_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNVFNFhmwKTa"
      },
      "source": [
        "## Communication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQuYuqDf8FsR"
      },
      "outputs": [],
      "source": [
        "def comm(data):\n",
        "  start = 0\n",
        "  end = 1\n",
        "  l = []\n",
        "  idx = []\n",
        "  num = data['Utterance end time (milliseconds)'].iloc[-1]/60000/20\n",
        "  for i in range(len(data)):\n",
        "    if data['Utterance end time (milliseconds)'].iloc[i] > (end*60000*num):\n",
        "      c = 0\n",
        "      for j in data['DA'].iloc[start:i+1]:\n",
        "        if j in ['Extra Domain Other','Extra Domain Question', 'Extra Domain Answer', 'Greeting']:\n",
        "          c = 1\n",
        "          break\n",
        "      l.append(c)\n",
        "      idx.append([start,i+1])\n",
        "      start = i+1\n",
        "      end = end+1\n",
        "  c=0\n",
        "  for j in data['DA'].iloc[start:len(data)-1]:\n",
        "      if j in ['Extra Domain Other','Extra Domain Question', 'Extra Domain Answer', 'Greeting']:\n",
        "        c = 1\n",
        "        break\n",
        "  l.append(c)\n",
        "  idx.append([start,i+1])\n",
        "\n",
        "  p = Counter(list(d['Speaker']))['student']/len(list(d['Speaker']))\n",
        "\n",
        "  total_comm_score=(sum(list(map(int, l)))/len(l))/2\n",
        "\n",
        "  step = round(data['Utterance end time (milliseconds)'].iloc[-1]/60000/20)\n",
        "  time = [f\"{i:02}\" for i in range(step, step*20+1, step)]\n",
        "\n",
        "  return (l,idx, total_comm_score, time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Grqf4zYdwNGS"
      },
      "source": [
        "## Instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11_B_C7msxJz",
        "outputId": "f1784a85-52cb-475a-bee5-dd0ea0345e6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "tokenizer_1 = AutoTokenizer.from_pretrained(\"s-nlp/xlmr_formality_classifier\")\n",
        "model_1 = AutoModelForSequenceClassification.from_pretrained(\"s-nlp/xlmr_formality_classifier\")\n",
        "config_1 = AutoConfig.from_pretrained(\"s-nlp/xlmr_formality_classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikLonWJ3Beas"
      },
      "outputs": [],
      "source": [
        "def instru(data):\n",
        "  l = []\n",
        "  for i in range(len(data)):\n",
        "    text = data.iloc[i]['Utterance']\n",
        "    encoded_input = tokenizer_1(text, return_tensors='pt')\n",
        "    output = model_1(**encoded_input)\n",
        "    scores = output[0][0].detach().numpy()\n",
        "    scores = softmax(scores)\n",
        "    # Print labels and scores\n",
        "    ranking = np.argsort(scores)\n",
        "    ranking = ranking[::-1]\n",
        "    l.append(config_1.id2label[ranking[0]])\n",
        "  data['formality'] = l\n",
        "  start = 0\n",
        "  end = 1\n",
        "  l = []\n",
        "  idx = []\n",
        "  lb = []\n",
        "  num = data['Utterance end time (milliseconds)'].iloc[-1]/60000/20\n",
        "  for i in range(len(data)):\n",
        "    if data['Utterance end time (milliseconds)'].iloc[i] > (end*60000*num):\n",
        "      c = 0\n",
        "      b = 0\n",
        "      for j in range(start,i+1):\n",
        "        if (data['DA'].iloc[j]=='Explanation') & (data['Speaker'].iloc[j]=='student'):\n",
        "          c += 1\n",
        "      for j in data['formality'].iloc[start:i+1]:\n",
        "        if j == 'informal':\n",
        "          b += 1\n",
        "      l.append(c)\n",
        "      lb.append(b)\n",
        "      idx.append([start,i+1])\n",
        "      start = i+1\n",
        "      end = end+1\n",
        "  c=0\n",
        "  b= 0\n",
        "  for j in range(start,len(data)-1):\n",
        "      if (data['DA'].iloc[j]=='Explanation') & (data['Speaker'].iloc[j]=='student'):\n",
        "        c += 1\n",
        "  for j in data['formality'].iloc[start:i+1]:\n",
        "      if j == 'informal':\n",
        "        b += 1\n",
        "  lb.append(b)\n",
        "  l.append(c)\n",
        "  idx.append([start,i+1])\n",
        "\n",
        "  total_instru_score = sum(list(map(int, l)))/len(data) - sum(list(map(int, lb)))/len(data)\n",
        "  return (lb,idx, total_instru_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iHZD933wQVL"
      },
      "source": [
        "## Social emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgNDhmcA1UDV",
        "outputId": "58567e50-3402-4d70-fd0a-a02b5db7a0f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "# Preprocess text (username and link placeholders)\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYAbuzYb2bxS"
      },
      "outputs": [],
      "source": [
        "def socialEmo(data):\n",
        "  l = []\n",
        "  for i in range(len(data)):\n",
        "    text = data.iloc[i]['Utterance']\n",
        "    text = preprocess(text)\n",
        "    encoded_input = tokenizer(text, return_tensors='pt')\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0][0].detach().numpy()\n",
        "    scores = softmax(scores)\n",
        "    # Print labels and scores\n",
        "    ranking = np.argsort(scores)\n",
        "    ranking = ranking[::-1]\n",
        "    l.append(config.id2label[ranking[0]])\n",
        "  data['tone'] = l\n",
        "  stu_neg = sum((data['tone']=='negative') & (data['Speaker']=='student'))/ sum( (data['Speaker']=='student'))\n",
        "  tutor_neg = sum((data['tone']=='negative') & (data['Speaker']=='student'))/ sum( (data['Speaker']=='tutor'))\n",
        "\n",
        "  start = 0\n",
        "  end = 1\n",
        "  l = []\n",
        "  idx = []\n",
        "  num = data['Utterance end time (milliseconds)'].iloc[-1]/60000/20\n",
        "  for i in range(len(data)):\n",
        "    if data['Utterance end time (milliseconds)'].iloc[i] > (end*60000*num):\n",
        "      c = 0\n",
        "      for j in data['tone'].iloc[start:i+1]:\n",
        "        if j == 'negative':\n",
        "          c += 1\n",
        "      l.append(c/len(data))\n",
        "      idx.append([start,i+1])\n",
        "      start = i+1\n",
        "      end = end+1\n",
        "    c=0\n",
        "  for j in data['tone'].iloc[start:len(data)-1]:\n",
        "      if j == 'negative':\n",
        "        c += 1\n",
        "  l.append(c/len(data))\n",
        "  idx.append([start,i+1])\n",
        "\n",
        "  return (stu_neg,tutor_neg, l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hvsj4A6wV6G"
      },
      "source": [
        "## Feedback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h65otpWBSoUq"
      },
      "outputs": [],
      "source": [
        "def feedback(data):\n",
        "  start = 0\n",
        "  end = 1\n",
        "  l = []\n",
        "  lb = []\n",
        "  idx = []\n",
        "  totalcount = 0\n",
        "  num = data['Utterance end time (milliseconds)'].iloc[-1]/60000/20\n",
        "  for i in range(len(data)):\n",
        "    if data['Utterance end time (milliseconds)'].iloc[i] > (end*60000*num):\n",
        "      c = 0\n",
        "      b = 0\n",
        "      for j in data['DA'].iloc[start:i+1]:\n",
        "        if j in ['Positive Feedback','Negative Feedback']:\n",
        "          totalcount +=1\n",
        "          if i+4 < len(data):\n",
        "            temp = data.iloc[i:i+4]['DA']\n",
        "            if ('Explanation' in list(temp)):\n",
        "              c += 1\n",
        "            else:\n",
        "              b += 1\n",
        "      l.append(c)\n",
        "      lb.append(b/len(data))\n",
        "      idx.append([start,i+1])\n",
        "      start = i+1\n",
        "      end = end+1\n",
        "  c = 0\n",
        "  b = 0\n",
        "  for j in data['DA'].iloc[start:len(data)-1]:\n",
        "      if j in ['Positive Feedback','Negative Feedback']:\n",
        "          totalcount+=1\n",
        "          if i+4 < len(data):\n",
        "            temp = data.iloc[i:i+4]['DA']\n",
        "            if ('Explanation' in list(temp)):\n",
        "              c += 1\n",
        "            else:\n",
        "              b +=1\n",
        "  l.append(c)\n",
        "  lb.append(b/len(data))\n",
        "  idx.append([start,i+1])\n",
        "\n",
        "  total_feedback_score =  sum(list(map(int, l)))/totalcount\n",
        "  return (lb,idx, total_feedback_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svAupYC88h2W"
      },
      "source": [
        "# Running Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ikq_SAlDYr8"
      },
      "outputs": [],
      "source": [
        "raw = pd.read_csv('/Data/500 Sample Supplementary Data (Laste edit_ Mar 21) - mar20 base tutoring session data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTFjjkZnxpsq"
      },
      "outputs": [],
      "source": [
        "dir = 'Directory to the DA labled transcripts'\n",
        "n=os.listdir(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKzdIRUA36jn",
        "outputId": "86da9a06-1c63-4896-92c5-d5c5d42e2030"
      },
      "outputs": [],
      "source": [
        "data = raw\n",
        "total_tech_score = []\n",
        "total_comm_score = []\n",
        "total_instru_score = []\n",
        "total_feedback_score = []\n",
        "stu_neg = []\n",
        "tutor_neg = []\n",
        "duration = []\n",
        "\n",
        "interval = []\n",
        "t_score = []\n",
        "c_score = []\n",
        "i_score = []\n",
        "f_score = []\n",
        "social_emo = []\n",
        "\n",
        "time = []\n",
        "\n",
        "r = pd.DataFrame()\n",
        "\n",
        "for i in n:\n",
        "  if i[:-4] in list(raw['session_uid']):\n",
        "    r = pd.concat([r, raw[raw['session_uid'] == i[:-4]]])\n",
        "\n",
        "    d = pd.read_csv(dir+i)\n",
        "\n",
        "    tech_score, tech_interval, s = tech(d)\n",
        "    total_tech_score.append(s)\n",
        "    t_score.append(tech_score)\n",
        "\n",
        "    comm_score, comm_interval, s, t = comm(d)\n",
        "    total_comm_score.append(s)\n",
        "    c_score.append(comm_score)\n",
        "\n",
        "    instru_score, instru_interval, s = instru(d)\n",
        "    total_instru_score.append(s)\n",
        "    i_score.append(instru_score)\n",
        "    print(instru_score)\n",
        "\n",
        "    feedback_score, feedback_interval, s = feedback(d)\n",
        "    total_feedback_score.append(s)\n",
        "    f_score.append(feedback_score)\n",
        "\n",
        "    stu, tu, s = socialEmo(d)\n",
        "    stu_neg.append(stu)\n",
        "    tutor_neg.append(tu)\n",
        "    social_emo.append(s)\n",
        "\n",
        "    interval.append(feedback_interval)\n",
        "\n",
        "    du = raw[raw['session_uid'] == i[:-4]]\n",
        "    duration.append(list(d['Utterance end time (milliseconds)'].iloc[-1]/60000/du['Sessions duration (min)'])[0])\n",
        "\n",
        "    time.append(t)\n",
        "\n",
        "r['total_tech_score'] = total_tech_score\n",
        "r['total_comm_score'] = total_comm_score\n",
        "r['total_instru_score'] = total_instru_score\n",
        "r['total_feedback_score'] = total_feedback_score\n",
        "r['stu_neg'] = stu_neg\n",
        "r['tutor_neg'] = tutor_neg\n",
        "r['duration'] = duration\n",
        "\n",
        "r['interval'] = interval\n",
        "r['t_score'] = t_score\n",
        "r['social_emo'] = social_emo\n",
        "r['c_score'] = c_score\n",
        "r['i_score'] = i_score\n",
        "r['f_score'] = f_score\n",
        "\n",
        "r['time'] = time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVbRGq-_OG3-"
      },
      "source": [
        "# Calculate Scores 0-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ES9CE4cTRgC"
      },
      "outputs": [],
      "source": [
        "r['total_tech'] = round(1- r['total_tech_score'],2)*10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5WdvYNATjO-"
      },
      "outputs": [],
      "source": [
        "r['total_comm'] = round(1- r['total_comm_score'],2)*10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYDzOFq1Z2O8"
      },
      "outputs": [],
      "source": [
        "r['socialemo'] = 5*r['stu_neg']+5*r['tutor_neg']\n",
        "m = np.mean(round(1-r['socialemo']*100,2))\n",
        "std = np.std(round(1-r['socialemo']*100,2))\n",
        "y = -(round(1-r['socialemo']*100,2)-m)/std/np.sqrt(2)\n",
        "r['total_socialemo'] = round(scipy.special.erfc(y, out=None)*5,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H6hrMMBVmWD"
      },
      "outputs": [],
      "source": [
        "m = np.mean(round(r['total_instru_score']*100,2))\n",
        "std = np.std(round(r['total_instru_score']*100,2))\n",
        "y = -(round(r['total_instru_score']*100,2)-m)/std/np.sqrt(2)\n",
        "r['total_instru'] = round(scipy.special.erfc(y, out=None)*5,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJTKJYkPT1-u"
      },
      "outputs": [],
      "source": [
        "r['total_feedback']=round(r['total_feedback_score']*10,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWO7CiT0ZWCt"
      },
      "outputs": [],
      "source": [
        "r['total'] = (r['total_tech'] +r['total_comm']+r['total_socialemo']+r['total_instru']+r['total_feedback'])/5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbUfOoIZauWE"
      },
      "outputs": [],
      "source": [
        "r.to_csv('Directory to save the output file')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2jtqsE5Nl8E"
      },
      "source": [
        "# Output Json Files for Frontend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfemAMN2d4Ms"
      },
      "source": [
        "## first page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQnwRaV1NnnR"
      },
      "outputs": [],
      "source": [
        "def jsondict(s):\n",
        "  risklist = []\n",
        "  score_label = {\"text\" : \"Low Rubric Rating\", \"color\": \"aquamarine\"}\n",
        "  tech_label = {\"text\" : \"Technical Issues\",\"color\": \"blue\"}\n",
        "  duration_label =  {\"text\" : \"Short Sessions\",\"color\": \"yellow\"}\n",
        "\n",
        "  dr = (s['duration'] * s['Sessions duration (min)']).astype('int64')\n",
        "  d = s['Sessions duration (min)']\n",
        "\n",
        "  if s['total_tech_score']>=0.3:\n",
        "    risklist.append(tech_label)\n",
        "  if s['total'] < 6.5:\n",
        "    risklist.append(score_label)\n",
        "  if s['duration']<0.8:\n",
        "    risklist.append(duration_label)\n",
        "  dict =   {\"sessionId\": s['session_uid'],\n",
        "            \"tutor\": s[\"Tutor Name\"],\n",
        "            \"subject\": s[\"subject\"],\n",
        "            \"date\": s[\"tutoring session occurred date\"],\n",
        "            \"duration\": str(dr) + '/' + str(d),\n",
        "            \"rubircRating\": s['total'],\n",
        "            \"sessionDuration\": s['duration'],\n",
        "            \"viewUrl\": \"https://google.com\",\n",
        "            \"riskAreas\": risklist}\n",
        "  return dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH2uW4buNp-r"
      },
      "outputs": [],
      "source": [
        "result = []\n",
        "for i in range(len(r)):\n",
        "  s = r.iloc[i]\n",
        "  result.append(jsondict(s))\n",
        "json_object = json.dumps(result, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pbe_BzGPOEYY"
      },
      "outputs": [],
      "source": [
        "with open(\"/Directory for Json Output/list_first_page.json\", \"w\") as outfile:\n",
        "    outfile.write(json_object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3cAhii_eH2i"
      },
      "source": [
        "## second page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qoklT9EC92A"
      },
      "outputs": [],
      "source": [
        "for i in range(len(r)):\n",
        "  s = r.iloc[i]\n",
        "  a = ast.literal_eval(s['time'])\n",
        "  a.insert(0, '00')\n",
        "  dict = {\n",
        "    \"communicationLevelInfo\": {\n",
        "      \"level\": round(r['total_comm'].iloc[i],1),\n",
        "      \"percent\": round(sum((r['total_comm']<=r['total_comm'].iloc[i]))/len(r['total_comm'])*100,1),\n",
        "      \"remarks\": [\n",
        "        \"· This score measures communication in the session in the session.\",\n",
        "        \"· Students are expected to actively participate in course content.\",\n",
        "        \"· The score is measured by the turns and extra domain discussion.\"\n",
        "      ]\n",
        "    },\n",
        "    \"instructionDeliveryInfo\": {\n",
        "      \"level\": round(r['total_instru'].iloc[i],1),\n",
        "      \"percent\": round(sum((r['total_instru']<=r['total_instru'].iloc[i]))/len(r['total_instru'])*100,1),\n",
        "      \"remarks\": [\n",
        "        \"· This score measures tutor's instruction techniques in the session.\",\n",
        "        \"· Turors are expected to check student understanding by ask them to explain.\",\n",
        "        \"· The score is measured by the frequency.\"\n",
        "      ]\n",
        "    },\n",
        "    \"techToolUsageInfo\": {\n",
        "      \"level\": round(r['total_tech'].iloc[i],1),\n",
        "      \"percent\": round(sum((r['total_tech']<=r['total_tech'].iloc[i]))/len(r['total_tech'])*100,1),\n",
        "      \"remarks\": [\n",
        "        \"· This score measures technical difficulties in the session.\",\n",
        "        \"· Tutors are expected to spend less time on technical difficulties.\",\n",
        "        \"· The score is measured by the time.\"\n",
        "      ]\n",
        "    },\n",
        "    \"socioEmotionalTeachingInfo\": {\n",
        "      \"level\": round(r['total_socialemo'].iloc[i],1),\n",
        "      \"percent\": round(sum((r['total_socialemo']<=r['total_socialemo'].iloc[i]))/len(r['total_socialemo'])*100,1),\n",
        "      \"remarks\": [\n",
        "        \"· This score measures technical difficulty in the session.\",\n",
        "        \"· Tutors and students are expected to use more positive tones.\",\n",
        "        \"· The score is measured by the proportion of negative tones.\"\n",
        "      ]\n",
        "    },\n",
        "    \"feedbackInfo\": {\n",
        "      \"level\": round(r['total_feedback'].iloc[i],1),\n",
        "      \"percent\": round(sum((r['total_feedback']<=r['total_feedback'].iloc[i]))/len(r['total_feedback'])*100,1),\n",
        "      \"remarks\": [\n",
        "        \"· This score measures tutor's feedback quality in the session.\",\n",
        "        \"· Tutors are expected to provide detailed feedback with explainations.\",\n",
        "        \"· The score is measured by the proportion of good feedback.\"\n",
        "      ]\n",
        "    },\n",
        "    \"timeline\": {\n",
        "      \"timeText\": a[:20],\n",
        "      \"communicationLevel\": [int(n)*2 for n in ast.literal_eval(r['c_score'].iloc[i])][:20],\n",
        "      \"instructionDelivery\": [max(round(n /( max(ast.literal_eval(r['i_score'].iloc[i]))/4)-1),0) for n in ast.literal_eval(r['i_score'].iloc[i])][:20],\n",
        "      \"techToolUsage\": [int(n)*2 for n in ast.literal_eval(r['t_score'].iloc[i])][:20],\n",
        "      \"socioEmotionalTeaching\": [max(round(n /( max(ast.literal_eval(r['social_emo'].iloc[i]))/4)-1),0) for n in ast.literal_eval(r['social_emo'].iloc[i])][:20],\n",
        "      \"feedback\": [max(round(n /( max(ast.literal_eval(r['f_score'].iloc[i]))/4)-1),0) for n in ast.literal_eval(r['f_score'].iloc[i])][:20]\n",
        "    }\n",
        "\n",
        "  }\n",
        "  json_object = json.dumps(dict, indent=2)\n",
        "  with open(\"/Directory for Json Output/detail_\"+ r['session_uid'].iloc[i] +\".json\", \"w\") as outfile:\n",
        "      outfile.write(json_object)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "00W7lv-MvmZP",
        "UGDBxu_nwHOo",
        "cNVFNFhmwKTa",
        "Grqf4zYdwNGS",
        "6iHZD933wQVL",
        "3Hvsj4A6wV6G",
        "oVbRGq-_OG3-",
        "G3cAhii_eH2i"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
